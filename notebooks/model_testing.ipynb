{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","import keras\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df=pd.read_csv('/kaggle/input/energy-demand-forecast/featured_power_consumption.csv')\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","statistics = df['Global_active_power'].agg(['mean', 'min', 'median', 'max'])\n","average=df['Global_active_power'].mean()\n","\n","print(statistics)\n","print('avg',average)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"Start Datetime:\", df.Datetime.min())\n","print(\"End Datetime:\",df.Datetime.max())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","\n","dataset = df.Global_active_power.values.reshape(-1,1)\n","\n","scaler=MinMaxScaler(feature_range=(0,1))\n","scaled_data=scaler.fit_transform(dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Assuming scaled_data is already prepared\n","\n","train_size = int(len(scaled_data) * 0.8)\n","train, test = scaled_data[:train_size], scaled_data[train_size:]\n","\n","# Now you can use the `train` and `test` datasets as shown in the previous code\n","look_back = 168\n","\n","# Prepare the training dataset\n","X_train, y_train = [], []\n","for i in range(len(train) - look_back - 1):\n","    X_train.append(train[i:(i + look_back), 0])\n","    y_train.append(train[i + look_back, 0])\n","X_train = np.array(X_train)\n","y_train = np.array(y_train)\n","\n","# Prepare the testing dataset\n","X_test, y_test = [], []\n","for i in range(len(test) - look_back - 1):\n","    X_test.append(test[i:(i + look_back), 0])\n","    y_test.append(test[i + look_back, 0])\n","X_test = np.array(X_test)\n","y_test = np.array(y_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["y_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X_train = np.reshape(X_train, (X_train.shape[0], look_back,1))\n","X_test = np.reshape(X_test, (X_test.shape[0], look_back ,1))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"Shape of X_train:\", X_train.shape)  # Should be (samples, look_back, 1)\n","print(\"Shape of X_test:\", X_test.shape)  "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, BatchNormalization, Flatten, AdditiveAttention, Multiply\n","\n","input_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))\n","\n","# First LSTM layer\n","lstm_1 = LSTM(units=50, return_sequences=True)(input_layer)\n","\n","# Batch Normalization after the first LSTM layer\n","batch_norm_1 = BatchNormalization()(lstm_1)\n","\n","# Second LSTM layer\n","lstm_2 = LSTM(units=50, return_sequences=True)(batch_norm_1)\n","\n","# Batch Normalization after the second LSTM layer\n","batch_norm_2 = BatchNormalization()(lstm_2)\n","\n","# Attention mechanism applied on lstm_2 output\n","attention = AdditiveAttention(name='attention_weight')\n","attention_output = attention([batch_norm_2, batch_norm_2])\n","\n","# Multiply the attention output with the LSTM output\n","multiply_layer = Multiply()([batch_norm_2, attention_output])\n","\n","# Flattening the output\n","flatten_layer = Flatten()(multiply_layer)\n","\n","# Dropout layer for regularization\n","dropout_layer = Dropout(0.3)(flatten_layer)\n","\n","# Output layer\n","output_layer = Dense(1)(dropout_layer)\n","\n","# Defining the model\n","model = Model(inputs=input_layer, outputs=output_layer)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.compile(optimizer='adam', loss='mean_squared_error',  metrics=['mae'])\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","import joblib\n","\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","X_train_scaled = scaler.fit_transform(scaled_data.reshape(-1, 1))\n","\n","# Define the callback to save the best model based on validation loss\n","model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min', verbose=1)\n","\n","# Early stopping callback to avoid overfitting\n","early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n","\n","# Reduce learning rate when validation loss plateaus\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6, verbose=1)\n","\n","# Combine the callbacks into a list\n","callbacks_list = [early_stopping, model_checkpoint, reduce_lr]\n","\n","# Train the model with the defined callbacks\n","#history = model.fit(X_train, y_train, epochs=20, batch_size=128, validation_split=0.1, callbacks=callbacks_list, verbose=1)\n","joblib.dump(scaler, 'scaler.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["'''\n","plt.plot(history.history['loss'], label='Training Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#test_loss = model.evaluate(X_test, y_test)\n","#print(\"Test Loss: \", test_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow.keras.models import load_model\n","\n","model =load_model('/kaggle/input/model/keras/default/1/best_model (1).keras')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["y_pred = model.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import numpy as np\n","\n","# Calculate metrics\n","mae = mean_absolute_error(y_test, y_pred)\n","mse = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","r2 = r2_score(y_test, y_pred)\n","\n","# Print results\n","print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n","print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n","print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n","print(f\"RÂ² Score: {r2:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Assuming `Y_test` contains actual values and `predictions` contains predicted values\n","\n","# Create a figure object with a desired figure size\n","plt.figure(figsize=(20,6))\n","\n","# Plot the actual values (assuming Y_test contains the actual time series data)\n","plt.plot(range(4500), y_test[:4500], marker='.', label=\"Actual\", color='purple')\n","# Plot the predicted values (assuming `predictions` is the array of model predictions)\n","plt.plot(range(4500), y_pred[:4500], '-', label=\"Prediction\", color='red')\n","\n","# Remove the top spines for a cleaner look\n","sns.despine(top=True)\n","\n","# Adjusting the subplot location\n","plt.subplots_adjust(left=0.07)\n","\n","# Label the y-axis\n","plt.ylabel('Global_active_power', size=14)\n","\n","# Label the x-axis (in your case, it might be time steps or indices)\n","plt.xlabel('Time step', size=14)\n","\n","# Adding a legend with a font size of 16\n","plt.legend(fontsize=16)\n","\n","# Display the plot\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Step 2: Flatten the predictions\n","predicted_values = y_pred.flatten()\n","\n","# Step 3: Select a smaller range (e.g., first 500 steps)\n","small_range = 2000\n","predicted_values_small = predicted_values[:small_range]\n","\n","# Step 4: Identify Peak Demand (top 5% highest predicted values in the small range)\n","peak_threshold = np.percentile(predicted_values_small, 95)\n","peak_demand_indices = np.where(predicted_values_small >= peak_threshold)[0]\n","\n","# Step 5: Identify Low Demand (bottom 5% lowest predicted values in the small range)\n","low_threshold = np.percentile(predicted_values_small, 5)\n","low_demand_indices = np.where(predicted_values_small <= low_threshold)[0]\n","\n","# Step 6: Plot the results\n","plt.figure(figsize=(12,6))\n","\n","# Plot predicted values for the small range\n","plt.plot(np.arange(small_range), predicted_values_small, label=\"Predicted Demand\", color='blue')\n","\n","# Highlight peak demand periods\n","plt.scatter(peak_demand_indices, predicted_values_small[peak_demand_indices], color='red', label='Peak Demand', zorder=5)\n","\n","# Highlight low demand periods\n","plt.scatter(low_demand_indices, predicted_values_small[low_demand_indices], color='green', label='Low Demand', zorder=5)\n","\n","# Add labels and title\n","plt.xlabel('Time Steps', fontsize=14)\n","plt.ylabel('Predicted Demand', fontsize=14)\n","plt.title('Energy Demand Prediction (First 500 Steps)', fontsize=16)\n","plt.legend(fontsize=12)\n","\n","# Display the plot\n","plt.show()\n","\n","# Step 7: Print the indices of peak and low demand in the small range\n","print(\"Peak Demand Indices (First 500 Steps):\", peak_demand_indices)\n","print(\"Low Demand Indices (First 500 Steps):\", low_demand_indices)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def plot_power_consumption(start_date, end_date):\n","    # Filter data for the specified date range\n","    mask = (df['Datetime'] >= start_date) & (df['Datetime'] <= end_date)\n","    filtered_data = df.loc[mask]\n","\n","    # Plot the power consumption\n","    plt.figure(figsize=(12, 6))\n","    plt.plot(filtered_data['Datetime'], filtered_data['Global_active_power'], label='Global Active Power', color='blue')\n","    plt.xlabel('Datetime')\n","    plt.ylabel('Power Consumption (kW)')\n","    plt.title(f'Power Consumption from {start_date} to {end_date}')\n","    plt.legend()\n","    plt.grid()\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Example usage: Provide the date range\n","plot_power_consumption('2007-01-01 00:23:00', '2007-01-05 00:01:00')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":6148119,"sourceId":9989975,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelId":174952,"modelInstanceId":152497,"sourceId":179014,"sourceType":"modelInstanceVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
